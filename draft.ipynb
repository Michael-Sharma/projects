{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d795451c-814c-4e38-9b91-50915b7b7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import datetime\n",
    "import functools\n",
    "from graphcast import autoregressive, casting, checkpoint, data_utils as du, graphcast, normalization, rollout\n",
    "import haiku as hk \n",
    "import isodate\n",
    "import jax\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pysolar.radiation import get_radiation_direct\n",
    "from pysolar.solar import get_altitude\n",
    "import pytz\n",
    "import scipy\n",
    "from typing import Dict\n",
    "import xarray\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898bfa96-096e-4ec2-935f-d9cafcd215fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:43:45,027 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-03-05 17:43:45,031 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    }
   ],
   "source": [
    "client = cdsapi.Client() # Making a connection to CDS, to fetch data. \n",
    "# The fields to be fetched from the single-level source. \n",
    "singlelevelfields = [\n",
    "                        '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind',\n",
    "                        '2m_temperature',\n",
    "                        'geopotential',\n",
    "                        'land_sea_mask',\n",
    "                        'mean_sea_level_pressure',\n",
    "                        'toa_incident_solar_radiation', \n",
    "                        'total_precipitation'\n",
    "                    ]\n",
    "\n",
    "# The fields to be fetched from the pressure-level source. \n",
    "pressurelevelfields = [\n",
    "                        'u_component_of_wind',\n",
    "                        'v_component_of_wind',\n",
    "                        'geopotential',\n",
    "                        'specific_humidity',\n",
    "                        'temperature',\n",
    "                        'vertical_velocity'\n",
    "                    ]\n",
    "\n",
    "# The 13 pressure levels.\n",
    "pressure_levels = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "\n",
    "# Initializing other required constants.\n",
    "pi = math.pi\n",
    "\n",
    "# There is a gap of 6 hours between each graphcast prediction.\n",
    "gap = 6\n",
    "\n",
    "# Predicting for 4 timestamps.\n",
    "predictions_steps = 4\n",
    "\n",
    "watts_to_joules = 3600\n",
    "\n",
    "# Timestamp of the first prediction.\n",
    "first_prediction = datetime.datetime(2024, 1, 1, 18, 0)  \n",
    "\n",
    "# Latitude range.\n",
    "lat_range = range(-180, 181, 1) \n",
    "\n",
    "# Longitude range. \n",
    "lon_range = range(0, 360, 1) \n",
    "\n",
    "# A utility function used for ease of coding. \n",
    "# Converting the variable to a datetime object.\n",
    "def toDatetime(dt) -> datetime.datetime:\n",
    "    if isinstance(dt, datetime.date) and isinstance(dt, datetime.datetime):\n",
    "        return dt\n",
    "    \n",
    "    elif isinstance(dt, datetime.date) and not isinstance(dt, datetime.datetime):\n",
    "        return datetime.datetime.combine(dt, datetime.datetime.min.time())\n",
    "    \n",
    "    elif isinstance(dt, str):\n",
    "        if 'T' in dt:\n",
    "            return isodate.parse_datetime(dt)\n",
    "        else:\n",
    "            return datetime.datetime.combine(isodate.parse_date(dt), datetime.datetime.min.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4191d8f-a47e-4ad1-b5d7-f5d137b21570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c21552-0281-4965-b906-e5e206808e5e",
   "metadata": {},
   "source": [
    "## Get inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9582e84e-6f7b-4590-97c4-0b73fae6ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:43:53,606 INFO Request ID is 16231e42-97bb-4430-ba5a-703c9192cf78\n",
      "2025-03-05 17:43:53,749 INFO status has been updated to accepted\n",
      "2025-03-05 17:44:02,426 INFO status has been updated to running\n",
      "2025-03-05 17:44:07,632 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a3d238c72422729e714428e4117e00e3.nc:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:44:10,771 INFO Request ID is 03482800-b2ee-4f39-9494-caa59ad15f27\n",
      "2025-03-05 17:44:10,888 INFO status has been updated to accepted\n",
      "2025-03-05 17:44:16,235 INFO status has been updated to running\n",
      "2025-03-05 17:44:19,726 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cb051f353118e9e20bc7187bbc06d21e.nc:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:44:26,239 INFO Request ID is 0ef960e1-cef8-41f7-84ef-96d3b0d247c4\n",
      "2025-03-05 17:44:26,358 INFO status has been updated to accepted\n",
      "2025-03-05 17:44:40,181 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c9670bee49fd495f1e01b95bb00aa8d9.nc:   0%|          | 0.00/19.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 2.76 s, total: 22.5 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting the single and pressure level values.\n",
    "def getSingleAndPressureValues():\n",
    "    # SINGLE LEVELS\n",
    "    dataset = 'reanalysis-era5-single-levels'\n",
    "\n",
    "    request_1 = {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': singlelevelfields[:6],\n",
    "                # 'variable': singlelevelfields[:5],\n",
    "                'grid': '1.0/1.0',\n",
    "                'year': [2024],\n",
    "                'month': [1],\n",
    "                'day': [1],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00', '03:00',\n",
    "                    '04:00', '05:00', '06:00', '07:00', \n",
    "                    '08:00', '09:00', '10:00', '11:00',\n",
    "                    '12:00'\n",
    "                ],\n",
    "                'data_format': 'netcdf'\n",
    "            }\n",
    "    \n",
    "    request_2 = {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': singlelevelfields[6:],\n",
    "                # 'variable': singlelevelfields[5:],\n",
    "                'grid': '1.0/1.0',\n",
    "                'year': [2024],\n",
    "                'month': [1],\n",
    "                'day': [1],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00', '03:00',\n",
    "                    '04:00', '05:00', '06:00', '07:00', \n",
    "                    '08:00', '09:00', '10:00', '11:00',\n",
    "                    '12:00'\n",
    "                ],\n",
    "                'data_format': 'netcdf'\n",
    "            }\n",
    "    \n",
    "    client.retrieve(\n",
    "            dataset,\n",
    "            request_1,\n",
    "            'single-level_1.nc'\n",
    "        )\n",
    "    \n",
    "    client.retrieve(\n",
    "            dataset,\n",
    "            request_2,\n",
    "            'single-level_2.nc'\n",
    "        )    \n",
    "    \n",
    "    # read data\n",
    "    singlelevel_1 = xarray.open_dataset('single-level_1.nc', engine = 'netcdf4').to_dataframe()\n",
    "    singlelevel_2 = xarray.open_dataset('single-level_2.nc', engine = 'netcdf4').to_dataframe()\n",
    "    \n",
    "    # drop useless columns\n",
    "    singlelevel_1 = singlelevel_1.drop(['number', 'expver'], axis=1)\n",
    "    singlelevel_2 = singlelevel_2.drop(['number', 'expver'], axis=1)\n",
    "    \n",
    "    # merge tables\n",
    "    singlelevel = singlelevel_1.merge(\n",
    "                                      singlelevel_2, \n",
    "                                      left_index=True, \n",
    "                                      right_index=True,\n",
    "                                      how='inner'\n",
    "                                  )\n",
    "    \n",
    "    del singlelevel_1, singlelevel_2\n",
    "    gc.collect()\n",
    "\n",
    "    singlelevel = singlelevel.rename(\n",
    "                                     columns = {\n",
    "                                         col:singlelevelfields[ind] for ind, col in enumerate(singlelevel.columns.values.tolist())\n",
    "                                     }\n",
    "                                 )\n",
    "    \n",
    "    singlelevel = singlelevel.rename(\n",
    "                                     columns = {\n",
    "                                         'geopotential': 'geopotential_at_surface'\n",
    "                                     }\n",
    "                                 )\n",
    "    \n",
    "    # Calculating the sum of the last 6 hours of rainfall. \n",
    "    singlelevel = singlelevel.sort_index()\n",
    "    \n",
    "    singlelevel['total_precipitation_6hr'] = (\n",
    "                                        singlelevel.groupby(level=[0,1])['total_precipitation']\n",
    "                                                   .rolling(window=6, min_periods=1)\n",
    "                                                   .sum()\n",
    "                                                   .reset_index(level=[0,1], drop=True)\n",
    "                                    )\n",
    "    \n",
    "    singlelevel.pop('total_precipitation')\n",
    "\n",
    "\n",
    "    \n",
    "    # PRESSURE LEVELS\n",
    "    dataset = 'reanalysis-era5-pressure-levels'\n",
    "    \n",
    "    request = {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': pressurelevelfields,\n",
    "            'grid': '1.0/1.0',\n",
    "            'year': [2024],\n",
    "            'month': [1],\n",
    "            'day': [1],\n",
    "            'time': ['06:00', '12:00'],\n",
    "            'pressure_level': pressure_levels,\n",
    "            'data_format': 'netcdf'\n",
    "        }\n",
    "\n",
    "    client.retrieve(\n",
    "        dataset,\n",
    "        request,\n",
    "        'pressure-level.nc'\n",
    "    )\n",
    "    \n",
    "    pressurelevel = xarray.open_dataset('pressure-level.nc', engine = 'netcdf4').to_dataframe()\\\n",
    "\n",
    "    # drop useless columns\n",
    "    pressurelevel = pressurelevel.drop(['number', 'expver'], axis=1)\n",
    "    \n",
    "    pressurelevel = pressurelevel.rename(\n",
    "                                         columns = {\n",
    "                                             col:pressurelevelfields[ind] for ind, col in enumerate(pressurelevel.columns.values.tolist())\n",
    "                                         }\n",
    "                                     )\n",
    "\n",
    "    # rename axis\n",
    "    singlelevel = singlelevel.rename_axis(index={'valid_time': 'time'})\n",
    "    pressurelevel = pressurelevel.rename_axis(index={'valid_time': 'time'})\n",
    "\n",
    "    return singlelevel, pressurelevel\n",
    "\n",
    "\n",
    "\n",
    "# Adding sin and cos of the year progress. \n",
    "def addYearProgress(secs, data):\n",
    "    progress = du.get_year_progress(secs)\n",
    "    data['year_progress_sin'] = math.sin(2 * pi * progress)\n",
    "    data['year_progress_cos'] = math.cos(2 * pi * progress)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Adding sin and cos of the day progress.\n",
    "def addDayProgress(secs, lon:str, data:pd.DataFrame):\n",
    "    lons = data.index.get_level_values(lon).unique()\n",
    "    progress:np.ndarray = du.get_day_progress(secs, np.array(lons))\n",
    "    prxlon = {lon:prog for lon, prog in list(zip(list(lons), progress.tolist()))}\n",
    "    data['day_progress_sin'] = data.index.get_level_values(lon).map(lambda x: math.sin(2 * pi * prxlon[x]))\n",
    "    data['day_progress_cos'] = data.index.get_level_values(lon).map(lambda x: math.cos(2 * pi * prxlon[x]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Adding day and year progress. \n",
    "def integrateProgress(data:pd.DataFrame):\n",
    "    for dt in data.index.get_level_values('time').unique():\n",
    "        seconds_since_epoch = toDatetime(dt).timestamp()\n",
    "        data = addYearProgress(seconds_since_epoch, data)\n",
    "        data = addDayProgress(\n",
    "                            seconds_since_epoch,\n",
    "                            'longitude' if 'longitude' in data.index.names else 'lon',\n",
    "                            data\n",
    "                        )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Adding batch field and renaming some others.\n",
    "def formatData(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.rename_axis(index = {'latitude': 'lat', 'longitude': 'lon'})\n",
    "    if 'batch' not in data.index.names:\n",
    "        data['batch'] = 0\n",
    "        data = data.set_index('batch', append = True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    values:Dict[str, xarray.Dataset] = {}\n",
    "    \n",
    "    single, pressure = getSingleAndPressureValues()\n",
    "    values['inputs'] = pd.merge(pressure, single, left_index = True, right_index = True, how = 'inner')\n",
    "    values['inputs'] = integrateProgress(values['inputs'])\n",
    "    values['inputs'] = formatData(values['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba46595-e34e-4c17-9ed8-3d9b3c1c700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4957ae8-2fc0-4303-b6e9-d65a1a2df260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e0ba084-adc0-4735-a8c0-6f8109f3bec2",
   "metadata": {},
   "source": [
    "## Get targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f41e19-e3c5-4588-9b9d-3f1ca4d5ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs also go here. \n",
    "predictionFields = [\n",
    "                        'u_component_of_wind',\n",
    "                        'v_component_of_wind',\n",
    "                        'geopotential',\n",
    "                        'specific_humidity',\n",
    "                        'temperature',\n",
    "                        'vertical_velocity',\n",
    "                        '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind',\n",
    "                        '2m_temperature',\n",
    "                        'mean_sea_level_pressure',\n",
    "                        'total_precipitation_6hr'\n",
    "                    ]\n",
    "\n",
    "\n",
    "# Creating an array full of nan values.\n",
    "def nans(*args) -> list:\n",
    "    return np.full((args), np.nan)\n",
    "\n",
    "\n",
    "# Adding or subtracting time.\n",
    "def deltaTime(dt, **delta) -> datetime.datetime:\n",
    "    return dt + datetime.timedelta(**delta)\n",
    "\n",
    "\n",
    "def getTargets(dt, data:pd.DataFrame):\n",
    "    # rename axis\n",
    "    data = data.rename_axis(index = {'pressure_level': 'level'})\n",
    "    \n",
    "    # Creating an array consisting of unique values of each index.\n",
    "    lat = sorted(data.index.get_level_values('lat').unique().tolist())\n",
    "    lon = sorted(data.index.get_level_values('lon').unique().tolist())\n",
    "    levels = sorted(data.index.get_level_values('level').unique().tolist())\n",
    "    batch = data.index.get_level_values('batch').unique().tolist()\n",
    "    \n",
    "    time = [deltaTime(dt, hours = days * gap) for days in range(4)]\n",
    "\n",
    "    # Creating an empty dataset using latitude, longitude, the pressure levels and each prediction timestamp.  \n",
    "    target = xarray.Dataset(\n",
    "        {\n",
    "            field: (\n",
    "                ['lat', 'lon', 'level', 'time'],\n",
    "                nans(len(lat), len(lon), len(levels), len(time))\n",
    "            ) for field in predictionFields\n",
    "        }, \n",
    "        coords = {\n",
    "            'lat': lat, \n",
    "            'lon': lon,\n",
    "            'level': levels,\n",
    "            'time': time, \n",
    "            'batch': batch\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return target.to_dataframe()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The code for creating inputs will be here. \n",
    "    values['targets'] = getTargets(first_prediction, values['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f3961-28a8-48eb-aa29-9f7f068b1c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2106b37d-cfbe-4e0d-a3ab-2567e42801c3",
   "metadata": {},
   "source": [
    "## Get forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19576762-96da-419b-890b-8a1612391bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 8s, sys: 2.31 s, total: 15min 10s\n",
      "Wall time: 15min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Includes the packages imported and constants assigned.\n",
    "# The functions created for the inputs and targets also go here. \n",
    "# Adding a timezone to datetime.datetime variables. \n",
    "def addTimezone(dt, tz = pytz.UTC) -> datetime.datetime:\n",
    "    dt = toDatetime(dt)\n",
    "    if dt.tzinfo == None:\n",
    "        return pytz.UTC.localize(dt).astimezone(tz)\n",
    "    else:\n",
    "        return dt.astimezone(tz)\n",
    "\n",
    "\n",
    "\n",
    "# Getting the solar radiation value wrt longitude, latitude and timestamp. \n",
    "def getSolarRadiation(longitude, latitude, dt):  \n",
    "    altitude_degrees = get_altitude(latitude, longitude, addTimezone(dt))\n",
    "    solar_radiation = get_radiation_direct(dt, altitude_degrees) if altitude_degrees > 0 else 0\n",
    "\n",
    "    return solar_radiation * watts_to_joules\n",
    "\n",
    "\n",
    "\n",
    "# Calculating the solar radiation values for timestamps to be predicted. \n",
    "def integrateSolarRadiation(data:pd.DataFrame):\n",
    "    dates = list(data.index.get_level_values('time').unique())\n",
    "    coords = [[lat, lon] for lat in lat_range for lon in lon_range]\n",
    "    values = []\n",
    "    \n",
    "    # For each data, getting the solar radiation value at a particular coordinate.     \n",
    "    for dt in dates:\n",
    "        values.extend(\n",
    "                      list(\n",
    "                          map(\n",
    "                              lambda coord: {\n",
    "                                  'time': dt,\n",
    "                                  'lon': coord[1],\n",
    "                                  'lat': coord[0],\n",
    "                                  'toa_incident_solar_radiation': getSolarRadiation(coord[1], coord[0], dt)\n",
    "                              },\n",
    "                              coords\n",
    "                          )\n",
    "                      )\n",
    "                  )\n",
    "  \n",
    "    # Setting indices.\n",
    "    values = pd.DataFrame(values).set_index(keys = ['lat', 'lon', 'time'])\n",
    "      \n",
    "    # The forcings dataset will now contain the solar radiation values.\n",
    "    return pd.merge(data, values, left_index = True, right_index = True, how = 'inner')\n",
    "\n",
    "\n",
    "\n",
    "def getForcings(data:pd.DataFrame):\n",
    "    # Since forcings data does not contain batch as an index, it is dropped.\n",
    "    # So are all the columns, since forcings data only has 5, which will be created.\n",
    "    forcingdf = data.reset_index(level = 'level', drop = True).drop(labels = predictionFields, axis = 1)\n",
    "    \n",
    "    # Keeping only the unique indices.\n",
    "    forcingdf = pd.DataFrame(index = forcingdf.index.drop_duplicates(keep = 'first'))\n",
    "\n",
    "    # Adding the sin and cos of day and year progress.\n",
    "    # Functions are included in the creation of inputs data section.\n",
    "    forcingdf = integrateProgress(forcingdf)\n",
    "\n",
    "    # Integrating the solar radiation values.\n",
    "    forcingdf = integrateSolarRadiation(forcingdf)\n",
    "\n",
    "    return forcingdf\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The code for creating inputs and targets will be here. \n",
    "    values['forcings'] = getForcings(values['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a88bec8-c58f-4be1-a59c-c21e2504468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f6975-2980-4127-963c-b8ca112c542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e407cbf5-c38e-4b0c-bd0b-e0e48e2190b3",
   "metadata": {},
   "source": [
    "## Postprocessing inputs, targets, forcings (transform to xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45375722-d372-4a2e-8e94-792aedfceee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs, targets and forcings also go here. \n",
    "# A dictionary created, containing each coordinate a data variable requires.\n",
    "class AssignCoordinates:\n",
    "    coordinates = {\n",
    "                    '2m_temperature': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'mean_sea_level_pressure': ['batch', 'lon', 'lat', 'time'],\n",
    "                    '10m_v_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n",
    "                    '10m_u_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'total_precipitation_6hr': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'temperature': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'geopotential': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'u_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'v_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'vertical_velocity': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'specific_humidity': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'toa_incident_solar_radiation': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'year_progress_cos': ['batch', 'time'],\n",
    "                    'year_progress_sin': ['batch', 'time'],\n",
    "                    'day_progress_cos': ['batch', 'lon', 'time'],\n",
    "                    'day_progress_sin': ['batch', 'lon', 'time'],\n",
    "                    'geopotential_at_surface': ['lon', 'lat'],\n",
    "                    'land_sea_mask': ['lon', 'lat'],\n",
    "                }\n",
    "\n",
    "def modifyCoordinates(data:xarray.Dataset):\n",
    "    \n",
    "    # Parsing through each data variable and removing unneeded indices.     \n",
    "    for var in list(data.data_vars):\n",
    "        varArray:xarray.DataArray = data[var]\n",
    "        nonIndices = list(set(list(varArray.coords)).difference(set(AssignCoordinates.coordinates[var])))\n",
    "        data[var] = varArray.isel(**{coord: 0 for coord in nonIndices})\n",
    "    data = data.drop_vars('batch')\n",
    "\n",
    "    return data\n",
    "\n",
    "def makeXarray(data:pd.DataFrame) -> xarray.Dataset:\n",
    "    \n",
    "    # Converting to xarray.\n",
    "    data = data.to_xarray()\n",
    "    data = modifyCoordinates(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # The code for creating inputs, targets and forcings will be here. \n",
    "    values = {value:makeXarray(values[value]) for value in values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58ebc61-1480-4942-aa38-5d0fc756b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467446e3-5769-4237-930b-3cfcaf2c8851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a2e63e4-1629-4ace-808e-4e7cb9c7a014",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b193c7-80d3-41b8-ae7c-09bd00599048",
   "metadata": {},
   "outputs": [],
   "source": [
    "gencast_mini = 'GenCast 1p0deg Mini _2019.npz'\n",
    "gencast_1deg = 'GenCast 1p0deg _2019.npz'\n",
    "gencast_025deg = 'GenCast 0p25deg _2019.npz'\n",
    "gencast_operational = 'GenCast 0p25deg Operational _2022.npz'\n",
    "\n",
    "\n",
    "graphcast_small = 'GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz'\n",
    "graphcast_operational = 'GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz'\n",
    "graphcast_34_lvls = 'GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f729df-0af0-4877-8490-a6893863d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/20250113/GenCast/real_time/graphcast/rollout.py:295: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_target_steps = targets_template.dims[\"time\"]\n",
      "/home/jovyan/20250113/GenCast/real_time/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  scan_length = targets_template.dims['time']\n",
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n",
      "/home/jovyan/20250113/GenCast/real_time/graphcast/autoregressive.py:115: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_inputs = inputs.dims['time']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scan body function carry input and carry output must have the same pytree structure, but they differ:\n\nThe input carry component inputs[0] is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]'), 'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,))}) but the corresponding component of the carry output is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,)), 'level': <xarray.IndexVariable 'level' (level: 13)> Size: 104B\narray([  50.,  100.,  150.,  200.,  250.,  300.,  400.,  500.,  600.,  700.,\n        850.,  925., 1000.]), 'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]')}), so the pytree node metadata does not match.\n\nRevise the function so that the carry output has the same pytree structure as the carry input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:143\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:119\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(cls, inputs, targets, forcings)\u001b[0m\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/rollout.py:233\u001b[0m, in \u001b[0;36mchunked_prediction\u001b[0;34m(predictor_fn, rng, inputs, targets_template, forcings, num_steps_per_chunk, verbose)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Outputs a long trajectory by iteratively concatenating chunked predictions.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m chunks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction_chunk \u001b[38;5;129;01min\u001b[39;00m chunked_prediction_generator(\n\u001b[1;32m    234\u001b[0m     predictor_fn\u001b[38;5;241m=\u001b[39mpredictor_fn,\n\u001b[1;32m    235\u001b[0m     rng\u001b[38;5;241m=\u001b[39mrng,\n\u001b[1;32m    236\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    237\u001b[0m     targets_template\u001b[38;5;241m=\u001b[39mtargets_template,\n\u001b[1;32m    238\u001b[0m     forcings\u001b[38;5;241m=\u001b[39mforcings,\n\u001b[1;32m    239\u001b[0m     num_steps_per_chunk\u001b[38;5;241m=\u001b[39mnum_steps_per_chunk,\n\u001b[1;32m    240\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose):\n\u001b[1;32m    241\u001b[0m   chunks_list\u001b[38;5;241m.\u001b[39mappend(jax\u001b[38;5;241m.\u001b[39mdevice_get(prediction_chunk))\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xarray\u001b[38;5;241m.\u001b[39mconcat(chunks_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/rollout.py:345\u001b[0m, in \u001b[0;36mchunked_prediction_generator\u001b[0;34m(predictor_fn, rng, inputs, targets_template, forcings, num_steps_per_chunk, verbose, pmap_devices)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Make predictions for the chunk.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m rng, this_rng \u001b[38;5;241m=\u001b[39m split_rng_fn(rng)\n\u001b[0;32m--> 345\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_targets_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforcings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_forcings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# In the pmapped case, profiling reveals that the predictions, forcings and\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# inputs are all copied onto a single TPU, causing OOM. To avoid this\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# we pull all of the input/output data off the devices. This will have\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# some performance impact, but maximise the memory efficiency.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# TODO(aelkadi): Pmap `_get_next_inputs` when running under pmap, and\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# remove the device_get.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pmap_devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<timed exec>:105\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(**kw)\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/transform.py:456\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(params\u001b[38;5;241m=\u001b[39mparams, state\u001b[38;5;241m=\u001b[39mstate, rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m<timed exec>:85\u001b[0m, in \u001b[0;36mrun_forward\u001b[0;34m(model_config, task_config, inputs, targets_template, forcings)\u001b[0m\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/autoregressive.py:212\u001b[0m, in \u001b[0;36mPredictor.__call__\u001b[0;34m(self, inputs, targets_template, forcings, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     one_step_prediction \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mremat(one_step_prediction)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Loop (without unroll) with hk states in cell (jax.lax.scan won't do).\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m _, flat_preds \u001b[38;5;241m=\u001b[39m \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_step_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# The result of scan will have an extra leading axis on all arrays,\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# corresponding to the target times in this case. We need to be prepared for\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# it when unflattening the arrays back into a Dataset:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m scan_result_template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    218\u001b[0m     target_template\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;241m.\u001b[39mexpand_dims(time\u001b[38;5;241m=\u001b[39mtargets_template\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/stateful.py:643\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length, reverse, unroll)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# We know that we don't need to thread params in and out, since for init we\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# have already created them (given that above we unroll one step of the scan)\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# and for apply we know they are immutable. As such we only need to thread the\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# state and rng in and out.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m init \u001b[38;5;241m=\u001b[39m (init, internal_state(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 643\u001b[0m (carry, state), ys \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstateful_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m update_internal_state(state)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_init_fn:\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:400\u001b[0m, in \u001b[0;36m_check_carry_type\u001b[0;34m(name, body_fun, in_carry, out_carry_tree, out_avals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m       differences \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m diffs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    399\u001b[0m                      \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiffs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 400\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m function carry input and carry output must have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytree structure, but they differ:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdifferences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevise the function so that the carry output has the same pytree \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructure as the carry input.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_map(core\u001b[38;5;241m.\u001b[39mtypematch, in_avals, out_avals)):\n\u001b[1;32m    407\u001b[0m   diffs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_aval\u001b[38;5;241m.\u001b[39mstr_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but the corresponding output carry component has type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    409\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_aval\u001b[38;5;241m.\u001b[39mstr_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_aval_mismatch_extra(in_aval,\u001b[38;5;250m \u001b[39mout_aval)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    410\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m path, in_aval, out_aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(paths, in_avals, out_avals)\n\u001b[1;32m    411\u001b[0m            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mtypematch(in_aval, out_aval)]\n",
      "\u001b[0;31mTypeError\u001b[0m: scan body function carry input and carry output must have the same pytree structure, but they differ:\n\nThe input carry component inputs[0] is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]'), 'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,))}) but the corresponding component of the carry output is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,)), 'level': <xarray.IndexVariable 'level' (level: 13)> Size: 104B\narray([  50.,  100.,  150.,  200.,  250.,  300.,  400.,  500.,  600.,  700.,\n        850.,  925., 1000.]), 'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]')}), so the pytree node metadata does not match.\n\nRevise the function so that the carry output has the same pytree structure as the carry input."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs, targets and forcings also go here. \n",
    "\n",
    "with open(\n",
    "    f'model/params/{graphcast_small}', 'rb'\n",
    ") as model:\n",
    "    ckpt = checkpoint.load(model, graphcast.CheckPoint)\n",
    "    params = ckpt.params\n",
    "    state = {}\n",
    "    model_config = ckpt.model_config\n",
    "    task_config = ckpt.task_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#!!!!!!!!DELETE!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "existing_array = params['grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0']['w']\n",
    "\n",
    "zeros_array = np.zeros((288, 512), dtype=np.float32)  \n",
    "\n",
    "new_array = np.vstack((existing_array, zeros_array))\n",
    "\n",
    "params['grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0']['w'] = new_array\n",
    "\n",
    "del existing_array, zeros_array, new_array\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "existing_array = params['grid2mesh_gnn/~_networks_builder/encoder_nodes_mesh_nodes_mlp/~/linear_0']['w']\n",
    "\n",
    "zeros_array = np.zeros((288, 512), dtype=np.float32)  \n",
    "\n",
    "new_array = np.vstack((existing_array, zeros_array))\n",
    "\n",
    "params['grid2mesh_gnn/~_networks_builder/encoder_nodes_mesh_nodes_mlp/~/linear_0']['w'] = new_array\n",
    "\n",
    "del existing_array, zeros_array, new_array\n",
    "gc.collect()\n",
    "\n",
    "#!!!!!!!!DELETE!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(r'model/stats/diffs_stddev_by_level.nc', 'rb') as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "with open(r'model/stats/mean_by_level.nc', 'rb') as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "with open(r'model/stats/stddev_by_level.nc', 'rb') as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config:graphcast.ModelConfig,\n",
    "    task_config:graphcast.TaskConfig\n",
    "):\n",
    "    predictor = graphcast.GraphCast(model_config, task_config)\n",
    "    \n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "    \n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "        predictor, diffs_stddev_by_level = \n",
    "        diffs_stddev_by_level, mean_by_level = \n",
    "        mean_by_level, stddev_by_level = stddev_by_level\n",
    "    )\n",
    "    \n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing = True)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    \n",
    "    return predictor(inputs, targets_template = targets_template, forcings = forcings)\n",
    "\n",
    "\n",
    "def with_configs(fn):\n",
    "    return functools.partial(\n",
    "                             fn, \n",
    "                             model_config = model_config,\n",
    "                             task_config = task_config\n",
    "                         )\n",
    "\n",
    "\n",
    "def with_params(fn):\n",
    "    return functools.partial(\n",
    "                             fn, \n",
    "                             params = params, \n",
    "                             state = state\n",
    "                         )\n",
    "\n",
    "\n",
    "def drop_state(fn):\n",
    "    return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    @classmethod\n",
    "    def predict(\n",
    "        cls,\n",
    "        inputs,\n",
    "        targets,\n",
    "        forcings\n",
    "    ) -> xarray.Dataset:\n",
    "        predictions = rollout.chunked_prediction(\n",
    "                                                 run_forward_jitted,\n",
    "                                                 rng = jax.random.PRNGKey(0),\n",
    "                                                 inputs = inputs,\n",
    "                                                 targets_template = targets,\n",
    "                                                 forcings = forcings\n",
    "                                             )\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # The code for creating inputs, targets, forcings & processing will be here. \n",
    "#     predictions = Predictor.predict(\n",
    "#                                     values['inputs'],\n",
    "#                                     values['targets'],\n",
    "#                                     values['forcings']\n",
    "#                                 )\n",
    "#     predictions.to_dataframe().to_csv('predictions.csv', sep = ',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The code for creating inputs, targets, forcings & processing will be here. \n",
    "predictions = Predictor.predict(\n",
    "                                values['inputs'],\n",
    "                                values['targets'],\n",
    "                                values['forcings']\n",
    "                            )\n",
    "# predictions.to_dataframe().to_csv('predictions.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03fa6ee2-9a88-49c2-809c-c29a56733b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2516d4e-81be-4b50-9734-522639e3e160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ffd45-668f-45c4-8b76-053292b18a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc585855-efff-493d-917e-5d72a69ff706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ebebea-b21d-4226-962e-4c511c20f8df",
   "metadata": {},
   "source": [
    "ValueError: 'grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0/w' with retrieved shape (186, 512) does not match shape=[474, 512] dtype=dtype(bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a30967-07ac-4ddc-ad0a-222dc49e96f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39ae85-ad21-4a0a-94a3-ffa0816373b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24700f-37ae-4a33-ba2a-85b61ee18283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a4ec8-87f8-4b31-b628-8440a890127a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146e536-031a-43f7-b857-85c9362c609b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb161d0-643a-44ce-9673-5c386ab3b914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d222fd-4e3f-4301-aaa4-e654793c235a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799f812-6dab-43eb-bf2f-10caf6109cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf3fdb-c375-4b0a-935c-a148ff2c8d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-gencast_env]",
   "language": "python",
   "name": "conda-env-.mlspace-gencast_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
