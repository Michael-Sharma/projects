{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d795451c-814c-4e38-9b91-50915b7b7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import datetime\n",
    "import functools\n",
    "from graphcast import autoregressive, casting, checkpoint, data_utils as du, graphcast, normalization, rollout\n",
    "import haiku as hk \n",
    "import isodate\n",
    "import jax\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pysolar.radiation import get_radiation_direct\n",
    "from pysolar.solar import get_altitude\n",
    "import pytz\n",
    "import scipy\n",
    "from typing import Dict\n",
    "import xarray\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898bfa96-096e-4ec2-935f-d9cafcd215fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:43:45,027 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-03-05 17:43:45,031 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    }
   ],
   "source": [
    "client = cdsapi.Client() # Making a connection to CDS, to fetch data. \n",
    "# The fields to be fetched from the single-level source. \n",
    "singlelevelfields = [\n",
    "                        '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind',\n",
    "                        '2m_temperature',\n",
    "                        'geopotential',\n",
    "                        'land_sea_mask',\n",
    "                        'mean_sea_level_pressure',\n",
    "                        'toa_incident_solar_radiation', \n",
    "                        'total_precipitation'\n",
    "                    ]\n",
    "\n",
    "# The fields to be fetched from the pressure-level source. \n",
    "pressurelevelfields = [\n",
    "                        'u_component_of_wind',\n",
    "                        'v_component_of_wind',\n",
    "                        'geopotential',\n",
    "                        'specific_humidity',\n",
    "                        'temperature',\n",
    "                        'vertical_velocity'\n",
    "                    ]\n",
    "\n",
    "# The 13 pressure levels.\n",
    "pressure_levels = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "\n",
    "# Initializing other required constants.\n",
    "pi = math.pi\n",
    "\n",
    "# There is a gap of 6 hours between each graphcast prediction.\n",
    "gap = 6\n",
    "\n",
    "# Predicting for 4 timestamps.\n",
    "predictions_steps = 4\n",
    "\n",
    "watts_to_joules = 3600\n",
    "\n",
    "# Timestamp of the first prediction.\n",
    "first_prediction = datetime.datetime(2024, 1, 1, 18, 0)  \n",
    "\n",
    "# Latitude range.\n",
    "lat_range = range(-180, 181, 1) \n",
    "\n",
    "# Longitude range. \n",
    "lon_range = range(0, 360, 1) \n",
    "\n",
    "# A utility function used for ease of coding. \n",
    "# Converting the variable to a datetime object.\n",
    "def toDatetime(dt) -> datetime.datetime:\n",
    "    if isinstance(dt, datetime.date) and isinstance(dt, datetime.datetime):\n",
    "        return dt\n",
    "    \n",
    "    elif isinstance(dt, datetime.date) and not isinstance(dt, datetime.datetime):\n",
    "        return datetime.datetime.combine(dt, datetime.datetime.min.time())\n",
    "    \n",
    "    elif isinstance(dt, str):\n",
    "        if 'T' in dt:\n",
    "            return isodate.parse_datetime(dt)\n",
    "        else:\n",
    "            return datetime.datetime.combine(isodate.parse_date(dt), datetime.datetime.min.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4191d8f-a47e-4ad1-b5d7-f5d137b21570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c21552-0281-4965-b906-e5e206808e5e",
   "metadata": {},
   "source": [
    "## Get inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9582e84e-6f7b-4590-97c4-0b73fae6ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:43:53,606 INFO Request ID is 16231e42-97bb-4430-ba5a-703c9192cf78\n",
      "2025-03-05 17:43:53,749 INFO status has been updated to accepted\n",
      "2025-03-05 17:44:02,426 INFO status has been updated to running\n",
      "2025-03-05 17:44:07,632 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a3d238c72422729e714428e4117e00e3.nc:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:44:10,771 INFO Request ID is 03482800-b2ee-4f39-9494-caa59ad15f27\n",
      "2025-03-05 17:44:10,888 INFO status has been updated to accepted\n",
      "2025-03-05 17:44:16,235 INFO status has been updated to running\n",
      "2025-03-05 17:44:19,726 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cb051f353118e9e20bc7187bbc06d21e.nc:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:44:26,239 INFO Request ID is 0ef960e1-cef8-41f7-84ef-96d3b0d247c4\n",
      "2025-03-05 17:44:26,358 INFO status has been updated to accepted\n",
      "2025-03-05 17:44:40,181 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c9670bee49fd495f1e01b95bb00aa8d9.nc:   0%|          | 0.00/19.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 2.76 s, total: 22.5 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting the single and pressure level values.\n",
    "def getSingleAndPressureValues():\n",
    "    # SINGLE LEVELS\n",
    "    dataset = 'reanalysis-era5-single-levels'\n",
    "\n",
    "    request_1 = {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': singlelevelfields[:6],\n",
    "                # 'variable': singlelevelfields[:5],\n",
    "                'grid': '1.0/1.0',\n",
    "                'year': [2024],\n",
    "                'month': [1],\n",
    "                'day': [1],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00', '03:00',\n",
    "                    '04:00', '05:00', '06:00', '07:00', \n",
    "                    '08:00', '09:00', '10:00', '11:00',\n",
    "                    '12:00'\n",
    "                ],\n",
    "                'data_format': 'netcdf'\n",
    "            }\n",
    "    \n",
    "    request_2 = {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': singlelevelfields[6:],\n",
    "                # 'variable': singlelevelfields[5:],\n",
    "                'grid': '1.0/1.0',\n",
    "                'year': [2024],\n",
    "                'month': [1],\n",
    "                'day': [1],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00', '03:00',\n",
    "                    '04:00', '05:00', '06:00', '07:00', \n",
    "                    '08:00', '09:00', '10:00', '11:00',\n",
    "                    '12:00'\n",
    "                ],\n",
    "                'data_format': 'netcdf'\n",
    "            }\n",
    "    \n",
    "    client.retrieve(\n",
    "            dataset,\n",
    "            request_1,\n",
    "            'single-level_1.nc'\n",
    "        )\n",
    "    \n",
    "    client.retrieve(\n",
    "            dataset,\n",
    "            request_2,\n",
    "            'single-level_2.nc'\n",
    "        )    \n",
    "    \n",
    "    # read data\n",
    "    singlelevel_1 = xarray.open_dataset('single-level_1.nc', engine = 'netcdf4').to_dataframe()\n",
    "    singlelevel_2 = xarray.open_dataset('single-level_2.nc', engine = 'netcdf4').to_dataframe()\n",
    "    \n",
    "    # drop useless columns\n",
    "    singlelevel_1 = singlelevel_1.drop(['number', 'expver'], axis=1)\n",
    "    singlelevel_2 = singlelevel_2.drop(['number', 'expver'], axis=1)\n",
    "    \n",
    "    # merge tables\n",
    "    singlelevel = singlelevel_1.merge(\n",
    "                                      singlelevel_2, \n",
    "                                      left_index=True, \n",
    "                                      right_index=True,\n",
    "                                      how='inner'\n",
    "                                  )\n",
    "    \n",
    "    del singlelevel_1, singlelevel_2\n",
    "    gc.collect()\n",
    "\n",
    "    singlelevel = singlelevel.rename(\n",
    "                                     columns = {\n",
    "                                         col:singlelevelfields[ind] for ind, col in enumerate(singlelevel.columns.values.tolist())\n",
    "                                     }\n",
    "                                 )\n",
    "    \n",
    "    singlelevel = singlelevel.rename(\n",
    "                                     columns = {\n",
    "                                         'geopotential': 'geopotential_at_surface'\n",
    "                                     }\n",
    "                                 )\n",
    "    \n",
    "    # Calculating the sum of the last 6 hours of rainfall. \n",
    "    singlelevel = singlelevel.sort_index()\n",
    "    \n",
    "    singlelevel['total_precipitation_6hr'] = (\n",
    "                                        singlelevel.groupby(level=[0,1])['total_precipitation']\n",
    "                                                   .rolling(window=6, min_periods=1)\n",
    "                                                   .sum()\n",
    "                                                   .reset_index(level=[0,1], drop=True)\n",
    "                                    )\n",
    "    \n",
    "    singlelevel.pop('total_precipitation')\n",
    "\n",
    "\n",
    "    \n",
    "    # PRESSURE LEVELS\n",
    "    dataset = 'reanalysis-era5-pressure-levels'\n",
    "    \n",
    "    request = {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': pressurelevelfields,\n",
    "            'grid': '1.0/1.0',\n",
    "            'year': [2024],\n",
    "            'month': [1],\n",
    "            'day': [1],\n",
    "            'time': ['06:00', '12:00'],\n",
    "            'pressure_level': pressure_levels,\n",
    "            'data_format': 'netcdf'\n",
    "        }\n",
    "\n",
    "    client.retrieve(\n",
    "        dataset,\n",
    "        request,\n",
    "        'pressure-level.nc'\n",
    "    )\n",
    "    \n",
    "    pressurelevel = xarray.open_dataset('pressure-level.nc', engine = 'netcdf4').to_dataframe()\\\n",
    "\n",
    "    # drop useless columns\n",
    "    pressurelevel = pressurelevel.drop(['number', 'expver'], axis=1)\n",
    "    \n",
    "    pressurelevel = pressurelevel.rename(\n",
    "                                         columns = {\n",
    "                                             col:pressurelevelfields[ind] for ind, col in enumerate(pressurelevel.columns.values.tolist())\n",
    "                                         }\n",
    "                                     )\n",
    "\n",
    "    # rename axis\n",
    "    singlelevel = singlelevel.rename_axis(index={'valid_time': 'time'})\n",
    "    pressurelevel = pressurelevel.rename_axis(index={'valid_time': 'time'})\n",
    "\n",
    "    return singlelevel, pressurelevel\n",
    "\n",
    "\n",
    "\n",
    "# Adding sin and cos of the year progress. \n",
    "def addYearProgress(secs, data):\n",
    "    progress = du.get_year_progress(secs)\n",
    "    data['year_progress_sin'] = math.sin(2 * pi * progress)\n",
    "    data['year_progress_cos'] = math.cos(2 * pi * progress)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Adding sin and cos of the day progress.\n",
    "def addDayProgress(secs, lon:str, data:pd.DataFrame):\n",
    "    lons = data.index.get_level_values(lon).unique()\n",
    "    progress:np.ndarray = du.get_day_progress(secs, np.array(lons))\n",
    "    prxlon = {lon:prog for lon, prog in list(zip(list(lons), progress.tolist()))}\n",
    "    data['day_progress_sin'] = data.index.get_level_values(lon).map(lambda x: math.sin(2 * pi * prxlon[x]))\n",
    "    data['day_progress_cos'] = data.index.get_level_values(lon).map(lambda x: math.cos(2 * pi * prxlon[x]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Adding day and year progress. \n",
    "def integrateProgress(data:pd.DataFrame):\n",
    "    for dt in data.index.get_level_values('time').unique():\n",
    "        seconds_since_epoch = toDatetime(dt).timestamp()\n",
    "        data = addYearProgress(seconds_since_epoch, data)\n",
    "        data = addDayProgress(\n",
    "                            seconds_since_epoch,\n",
    "                            'longitude' if 'longitude' in data.index.names else 'lon',\n",
    "                            data\n",
    "                        )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Adding batch field and renaming some others.\n",
    "def formatData(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.rename_axis(index = {'latitude': 'lat', 'longitude': 'lon'})\n",
    "    if 'batch' not in data.index.names:\n",
    "        data['batch'] = 0\n",
    "        data = data.set_index('batch', append = True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    values:Dict[str, xarray.Dataset] = {}\n",
    "    \n",
    "    single, pressure = getSingleAndPressureValues()\n",
    "    values['inputs'] = pd.merge(pressure, single, left_index = True, right_index = True, how = 'inner')\n",
    "    values['inputs'] = integrateProgress(values['inputs'])\n",
    "    values['inputs'] = formatData(values['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba46595-e34e-4c17-9ed8-3d9b3c1c700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4957ae8-2fc0-4303-b6e9-d65a1a2df260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e0ba084-adc0-4735-a8c0-6f8109f3bec2",
   "metadata": {},
   "source": [
    "## Get targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f41e19-e3c5-4588-9b9d-3f1ca4d5ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs also go here. \n",
    "predictionFields = [\n",
    "                        'u_component_of_wind',\n",
    "                        'v_component_of_wind',\n",
    "                        'geopotential',\n",
    "                        'specific_humidity',\n",
    "                        'temperature',\n",
    "                        'vertical_velocity',\n",
    "                        '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind',\n",
    "                        '2m_temperature',\n",
    "                        'mean_sea_level_pressure',\n",
    "                        'total_precipitation_6hr'\n",
    "                    ]\n",
    "\n",
    "\n",
    "# Creating an array full of nan values.\n",
    "def nans(*args) -> list:\n",
    "    return np.full((args), np.nan)\n",
    "\n",
    "\n",
    "# Adding or subtracting time.\n",
    "def deltaTime(dt, **delta) -> datetime.datetime:\n",
    "    return dt + datetime.timedelta(**delta)\n",
    "\n",
    "\n",
    "def getTargets(dt, data:pd.DataFrame):\n",
    "    # rename axis\n",
    "    data = data.rename_axis(index = {'pressure_level': 'level'})\n",
    "    \n",
    "    # Creating an array consisting of unique values of each index.\n",
    "    lat = sorted(data.index.get_level_values('lat').unique().tolist())\n",
    "    lon = sorted(data.index.get_level_values('lon').unique().tolist())\n",
    "    levels = sorted(data.index.get_level_values('level').unique().tolist())\n",
    "    batch = data.index.get_level_values('batch').unique().tolist()\n",
    "    \n",
    "    time = [deltaTime(dt, hours = days * gap) for days in range(4)]\n",
    "\n",
    "    # Creating an empty dataset using latitude, longitude, the pressure levels and each prediction timestamp.  \n",
    "    target = xarray.Dataset(\n",
    "        {\n",
    "            field: (\n",
    "                ['lat', 'lon', 'level', 'time'],\n",
    "                nans(len(lat), len(lon), len(levels), len(time))\n",
    "            ) for field in predictionFields\n",
    "        }, \n",
    "        coords = {\n",
    "            'lat': lat, \n",
    "            'lon': lon,\n",
    "            'level': levels,\n",
    "            'time': time, \n",
    "            'batch': batch\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return target.to_dataframe()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The code for creating inputs will be here. \n",
    "    values['targets'] = getTargets(first_prediction, values['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f3961-28a8-48eb-aa29-9f7f068b1c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2106b37d-cfbe-4e0d-a3ab-2567e42801c3",
   "metadata": {},
   "source": [
    "## Get forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19576762-96da-419b-890b-8a1612391bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 8s, sys: 2.31 s, total: 15min 10s\n",
      "Wall time: 15min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Includes the packages imported and constants assigned.\n",
    "# The functions created for the inputs and targets also go here. \n",
    "# Adding a timezone to datetime.datetime variables. \n",
    "def addTimezone(dt, tz = pytz.UTC) -> datetime.datetime:\n",
    "    dt = toDatetime(dt)\n",
    "    if dt.tzinfo == None:\n",
    "        return pytz.UTC.localize(dt).astimezone(tz)\n",
    "    else:\n",
    "        return dt.astimezone(tz)\n",
    "\n",
    "\n",
    "\n",
    "# Getting the solar radiation value wrt longitude, latitude and timestamp. \n",
    "def getSolarRadiation(longitude, latitude, dt):  \n",
    "    altitude_degrees = get_altitude(latitude, longitude, addTimezone(dt))\n",
    "    solar_radiation = get_radiation_direct(dt, altitude_degrees) if altitude_degrees > 0 else 0\n",
    "\n",
    "    return solar_radiation * watts_to_joules\n",
    "\n",
    "\n",
    "\n",
    "# Calculating the solar radiation values for timestamps to be predicted. \n",
    "def integrateSolarRadiation(data:pd.DataFrame):\n",
    "    dates = list(data.index.get_level_values('time').unique())\n",
    "    coords = [[lat, lon] for lat in lat_range for lon in lon_range]\n",
    "    values = []\n",
    "    \n",
    "    # For each data, getting the solar radiation value at a particular coordinate.     \n",
    "    for dt in dates:\n",
    "        values.extend(\n",
    "                      list(\n",
    "                          map(\n",
    "                              lambda coord: {\n",
    "                                  'time': dt,\n",
    "                                  'lon': coord[1],\n",
    "                                  'lat': coord[0],\n",
    "                                  'toa_incident_solar_radiation': getSolarRadiation(coord[1], coord[0], dt)\n",
    "                              },\n",
    "                              coords\n",
    "                          )\n",
    "                      )\n",
    "                  )\n",
    "  \n",
    "    # Setting indices.\n",
    "    values = pd.DataFrame(values).set_index(keys = ['lat', 'lon', 'time'])\n",
    "      \n",
    "    # The forcings dataset will now contain the solar radiation values.\n",
    "    return pd.merge(data, values, left_index = True, right_index = True, how = 'inner')\n",
    "\n",
    "\n",
    "\n",
    "def getForcings(data:pd.DataFrame):\n",
    "    # Since forcings data does not contain batch as an index, it is dropped.\n",
    "    # So are all the columns, since forcings data only has 5, which will be created.\n",
    "    forcingdf = data.reset_index(level = 'level', drop = True).drop(labels = predictionFields, axis = 1)\n",
    "    \n",
    "    # Keeping only the unique indices.\n",
    "    forcingdf = pd.DataFrame(index = forcingdf.index.drop_duplicates(keep = 'first'))\n",
    "\n",
    "    # Adding the sin and cos of day and year progress.\n",
    "    # Functions are included in the creation of inputs data section.\n",
    "    forcingdf = integrateProgress(forcingdf)\n",
    "\n",
    "    # Integrating the solar radiation values.\n",
    "    forcingdf = integrateSolarRadiation(forcingdf)\n",
    "\n",
    "    return forcingdf\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The code for creating inputs and targets will be here. \n",
    "    values['forcings'] = getForcings(values['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a88bec8-c58f-4be1-a59c-c21e2504468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f6975-2980-4127-963c-b8ca112c542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e407cbf5-c38e-4b0c-bd0b-e0e48e2190b3",
   "metadata": {},
   "source": [
    "## Postprocessing inputs, targets, forcings (transform to xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45375722-d372-4a2e-8e94-792aedfceee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs, targets and forcings also go here. \n",
    "# A dictionary created, containing each coordinate a data variable requires.\n",
    "class AssignCoordinates:\n",
    "    coordinates = {\n",
    "                    '2m_temperature': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'mean_sea_level_pressure': ['batch', 'lon', 'lat', 'time'],\n",
    "                    '10m_v_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n",
    "                    '10m_u_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'total_precipitation_6hr': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'temperature': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'geopotential': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'u_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'v_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'vertical_velocity': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'specific_humidity': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'toa_incident_solar_radiation': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'year_progress_cos': ['batch', 'time'],\n",
    "                    'year_progress_sin': ['batch', 'time'],\n",
    "                    'day_progress_cos': ['batch', 'lon', 'time'],\n",
    "                    'day_progress_sin': ['batch', 'lon', 'time'],\n",
    "                    'geopotential_at_surface': ['lon', 'lat'],\n",
    "                    'land_sea_mask': ['lon', 'lat'],\n",
    "                }\n",
    "\n",
    "def modifyCoordinates(data:xarray.Dataset):\n",
    "    \n",
    "    # Parsing through each data variable and removing unneeded indices.     \n",
    "    for var in list(data.data_vars):\n",
    "        varArray:xarray.DataArray = data[var]\n",
    "        nonIndices = list(set(list(varArray.coords)).difference(set(AssignCoordinates.coordinates[var])))\n",
    "        data[var] = varArray.isel(**{coord: 0 for coord in nonIndices})\n",
    "    data = data.drop_vars('batch')\n",
    "\n",
    "    return data\n",
    "\n",
    "def makeXarray(data:pd.DataFrame) -> xarray.Dataset:\n",
    "    \n",
    "    # Converting to xarray.\n",
    "    data = data.to_xarray()\n",
    "    data = modifyCoordinates(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # The code for creating inputs, targets and forcings will be here. \n",
    "    values = {value:makeXarray(values[value]) for value in values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58ebc61-1480-4942-aa38-5d0fc756b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467446e3-5769-4237-930b-3cfcaf2c8851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a2e63e4-1629-4ace-808e-4e7cb9c7a014",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b193c7-80d3-41b8-ae7c-09bd00599048",
   "metadata": {},
   "outputs": [],
   "source": [
    "gencast_mini = 'GenCast 1p0deg Mini _2019.npz'\n",
    "gencast_1deg = 'GenCast 1p0deg _2019.npz'\n",
    "gencast_025deg = 'GenCast 0p25deg _2019.npz'\n",
    "gencast_operational = 'GenCast 0p25deg Operational _2022.npz'\n",
    "\n",
    "\n",
    "graphcast_small = 'GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz'\n",
    "graphcast_operational = 'GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz'\n",
    "graphcast_34_lvls = 'GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08a4b0-c0c0-413d-9a4b-c78e73efce49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c02ad7d-b559-4aca-be3d-4771c29f6f1e",
   "metadata": {},
   "source": [
    "#### without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a3882e-06a5-4d78-8480-e2bd36712368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/20250113/GenCast/real_time/graphcast/rollout.py:295: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_target_steps = targets_template.dims[\"time\"]\n",
      "/home/jovyan/20250113/GenCast/real_time/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  scan_length = targets_template.dims['time']\n",
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0/w' with retrieved shape (186, 512) does not match shape=[474, 512] dtype=dtype(bfloat16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:107\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:83\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(cls, inputs, targets, forcings)\u001b[0m\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/rollout.py:233\u001b[0m, in \u001b[0;36mchunked_prediction\u001b[0;34m(predictor_fn, rng, inputs, targets_template, forcings, num_steps_per_chunk, verbose)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Outputs a long trajectory by iteratively concatenating chunked predictions.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m chunks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction_chunk \u001b[38;5;129;01min\u001b[39;00m chunked_prediction_generator(\n\u001b[1;32m    234\u001b[0m     predictor_fn\u001b[38;5;241m=\u001b[39mpredictor_fn,\n\u001b[1;32m    235\u001b[0m     rng\u001b[38;5;241m=\u001b[39mrng,\n\u001b[1;32m    236\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    237\u001b[0m     targets_template\u001b[38;5;241m=\u001b[39mtargets_template,\n\u001b[1;32m    238\u001b[0m     forcings\u001b[38;5;241m=\u001b[39mforcings,\n\u001b[1;32m    239\u001b[0m     num_steps_per_chunk\u001b[38;5;241m=\u001b[39mnum_steps_per_chunk,\n\u001b[1;32m    240\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose):\n\u001b[1;32m    241\u001b[0m   chunks_list\u001b[38;5;241m.\u001b[39mappend(jax\u001b[38;5;241m.\u001b[39mdevice_get(prediction_chunk))\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xarray\u001b[38;5;241m.\u001b[39mconcat(chunks_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/rollout.py:345\u001b[0m, in \u001b[0;36mchunked_prediction_generator\u001b[0;34m(predictor_fn, rng, inputs, targets_template, forcings, num_steps_per_chunk, verbose, pmap_devices)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Make predictions for the chunk.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m rng, this_rng \u001b[38;5;241m=\u001b[39m split_rng_fn(rng)\n\u001b[0;32m--> 345\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_targets_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforcings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_forcings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# In the pmapped case, profiling reveals that the predictions, forcings and\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# inputs are all copied onto a single TPU, causing OOM. To avoid this\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# we pull all of the input/output data off the devices. This will have\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# some performance impact, but maximise the memory efficiency.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# TODO(aelkadi): Pmap `_get_next_inputs` when running under pmap, and\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# remove the device_get.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pmap_devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<timed exec>:69\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(**kw)\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/transform.py:456\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(params\u001b[38;5;241m=\u001b[39mparams, state\u001b[38;5;241m=\u001b[39mstate, rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m<timed exec>:49\u001b[0m, in \u001b[0;36mrun_forward\u001b[0;34m(model_config, task_config, inputs, targets_template, forcings)\u001b[0m\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/autoregressive.py:212\u001b[0m, in \u001b[0;36mPredictor.__call__\u001b[0;34m(self, inputs, targets_template, forcings, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     one_step_prediction \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mremat(one_step_prediction)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Loop (without unroll) with hk states in cell (jax.lax.scan won't do).\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m _, flat_preds \u001b[38;5;241m=\u001b[39m \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_step_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# The result of scan will have an extra leading axis on all arrays,\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# corresponding to the target times in this case. We need to be prepared for\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# it when unflattening the arrays back into a Dataset:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m scan_result_template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    218\u001b[0m     target_template\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;241m.\u001b[39mexpand_dims(time\u001b[38;5;241m=\u001b[39mtargets_template\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/stateful.py:643\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length, reverse, unroll)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# We know that we don't need to thread params in and out, since for init we\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# have already created them (given that above we unroll one step of the scan)\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# and for apply we know they are immutable. As such we only need to thread the\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# state and rng in and out.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m init \u001b[38;5;241m=\u001b[39m (init, internal_state(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 643\u001b[0m (carry, state), ys \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstateful_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m update_internal_state(state)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_init_fn:\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/stateful.py:626\u001b[0m, in \u001b[0;36mscan.<locals>.stateful_fun\u001b[0;34m(carry, x)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temporary_internal_state(state):\n\u001b[1;32m    624\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39massert_no_new_parameters(), \\\n\u001b[1;32m    625\u001b[0m        base\u001b[38;5;241m.\u001b[39mpush_jax_trace_level():\n\u001b[0;32m--> 626\u001b[0m     carry, out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m   reserve_up_to_full_rng_block()\n\u001b[1;32m    628\u001b[0m   carry \u001b[38;5;241m=\u001b[39m (carry, internal_state(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/autoregressive.py:183\u001b[0m, in \u001b[0;36mPredictor.__call__.<locals>.one_step_prediction\u001b[0;34m(inputs, scan_variables)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Add constant inputs:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m all_inputs \u001b[38;5;241m=\u001b[39m xarray\u001b[38;5;241m.\u001b[39mmerge([constant_inputs, inputs])\n\u001b[0;32m--> 183\u001b[0m predictions: xarray\u001b[38;5;241m.\u001b[39mDataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforcings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforcings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m next_frame \u001b[38;5;241m=\u001b[39m xarray\u001b[38;5;241m.\u001b[39mmerge([predictions, forcings])\n\u001b[1;32m    189\u001b[0m next_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inputs(inputs, next_frame)\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/normalization.py:156\u001b[0m, in \u001b[0;36mInputsAndResiduals.__call__\u001b[0;34m(self, inputs, targets_template, forcings, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m norm_inputs \u001b[38;5;241m=\u001b[39m normalize(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scales, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locations)\n\u001b[1;32m    155\u001b[0m norm_forcings \u001b[38;5;241m=\u001b[39m normalize(forcings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scales, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_locations)\n\u001b[0;32m--> 156\u001b[0m norm_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforcings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_forcings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xarray_tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m pred: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unnormalize_prediction_and_add_input(inputs, pred),\n\u001b[1;32m    160\u001b[0m     norm_predictions)\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/casting.py:55\u001b[0m, in \u001b[0;36mBfloat16Cast.__call__\u001b[0;34m(self, inputs, targets_template, forcings, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor(inputs, targets_template, forcings, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bfloat16_variable_view():\n\u001b[0;32m---> 55\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_all_inputs_to_bfloat16\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforcings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m predictions_dtype \u001b[38;5;241m=\u001b[39m infer_floating_dtype(predictions)  \u001b[38;5;66;03m# pytype: disable=wrong-arg-types\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions_dtype \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mbfloat16:\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/graphcast.py:373\u001b[0m, in \u001b[0;36mGraphCast.__call__\u001b[0;34m(self, inputs, targets_template, forcings, is_training)\u001b[0m\n\u001b[1;32m    368\u001b[0m grid_node_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs_to_grid_node_features(inputs, forcings)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Transfer data for the grid to the mesh,\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# [num_mesh_nodes, batch, latent_size], [num_grid_nodes, batch, latent_size]\u001b[39;00m\n\u001b[1;32m    372\u001b[0m (latent_mesh_nodes, latent_grid_nodes\n\u001b[0;32m--> 373\u001b[0m  ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_grid2mesh_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_node_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Run message passing in the multimesh.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# [num_mesh_nodes, batch, latent_size]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m updated_latent_mesh_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_mesh_gnn(latent_mesh_nodes)\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/graphcast.py:660\u001b[0m, in \u001b[0;36mGraphCast._run_grid2mesh_gnn\u001b[0;34m(self, grid_node_features)\u001b[0m\n\u001b[1;32m    652\u001b[0m input_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grid2mesh_graph_structure\u001b[38;5;241m.\u001b[39m_replace(\n\u001b[1;32m    653\u001b[0m     edges\u001b[38;5;241m=\u001b[39m{grid2mesh_edges_key: new_edges},\n\u001b[1;32m    654\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_grid_nodes,\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmesh_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_mesh_nodes\n\u001b[1;32m    657\u001b[0m     })\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# Run the GNN.\u001b[39;00m\n\u001b[0;32m--> 660\u001b[0m grid2mesh_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grid2mesh_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m latent_mesh_nodes \u001b[38;5;241m=\u001b[39m grid2mesh_out\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmesh_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m    662\u001b[0m latent_grid_nodes \u001b[38;5;241m=\u001b[39m grid2mesh_out\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:464\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:305\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 305\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    309\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    310\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    311\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/deep_typed_graph_net.py:190\u001b[0m, in \u001b[0;36mDeepTypedGraphNet.__call__\u001b[0;34m(self, input_graph, global_norm_conditioning)\u001b[0m\n\u001b[1;32m    185\u001b[0m embedder_network, processor_networks, decoder_network \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_networks_builder(input_graph, global_norm_conditioning)\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Embed input features (if applicable).\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m latent_graph_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder_network\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Do `m` message passing steps in the latent graphs.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m latent_graph_m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process(latent_graph_0, processor_networks)\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:464\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:305\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 305\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    309\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    310\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    311\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/deep_typed_graph_net.py:353\u001b[0m, in \u001b[0;36mDeepTypedGraphNet._embed\u001b[0;34m(self, input_graph, embedder_network)\u001b[0m\n\u001b[1;32m    348\u001b[0m   input_graph \u001b[38;5;241m=\u001b[39m input_graph\u001b[38;5;241m.\u001b[39m_replace(\n\u001b[1;32m    349\u001b[0m       nodes\u001b[38;5;241m=\u001b[39mnew_nodes,\n\u001b[1;32m    350\u001b[0m       context\u001b[38;5;241m=\u001b[39minput_graph\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39m_replace(features\u001b[38;5;241m=\u001b[39m()))\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Embeds the node and edge features.\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m latent_graph_0 \u001b[38;5;241m=\u001b[39m \u001b[43membedder_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latent_graph_0\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/typed_graph_net.py:307\u001b[0m, in \u001b[0;36mGraphMapFeatures.<locals>._embed\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m node_set_key, embed_fn \u001b[38;5;129;01min\u001b[39;00m embed_node_fn\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    305\u001b[0m     node_set \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mnodes[node_set_key]\n\u001b[1;32m    306\u001b[0m     updated_nodes[node_set_key] \u001b[38;5;241m=\u001b[39m node_set\u001b[38;5;241m.\u001b[39m_replace(\n\u001b[0;32m--> 307\u001b[0m         features\u001b[38;5;241m=\u001b[39m\u001b[43membed_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    309\u001b[0m updated_context \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcontext\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embed_global_fn:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/jraph/_src/utils.py:828\u001b[0m, in \u001b[0;36mconcatenated_args.<locals>._decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m combined_args \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mtree_flatten(args)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tree\u001b[38;5;241m.\u001b[39mtree_flatten(kwargs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    827\u001b[0m concat_args \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate(combined_args, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:464\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:305\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 305\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    309\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    310\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    311\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/basic.py:126\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    125\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     out \u001b[38;5;241m=\u001b[39m layer(out)\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/jraph/_src/utils.py:828\u001b[0m, in \u001b[0;36mconcatenated_args.<locals>._decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m combined_args \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mtree_flatten(args)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tree\u001b[38;5;241m.\u001b[39mtree_flatten(kwargs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    827\u001b[0m concat_args \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate(combined_args, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:464\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:305\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 305\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    309\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    310\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    311\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/nets/mlp.py:115\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, inputs, dropout_rate, rng)\u001b[0m\n\u001b[1;32m    113\u001b[0m out \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 115\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m (num_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivate_final:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Only perform dropout if we are activating the output.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:464\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/module.py:305\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 305\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    309\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    310\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    311\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/basic.py:179\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, inputs, precision)\u001b[0m\n\u001b[1;32m    177\u001b[0m   stddev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size)\n\u001b[1;32m    178\u001b[0m   w_init \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mTruncatedNormal(stddev\u001b[38;5;241m=\u001b[39mstddev)\n\u001b[0;32m--> 179\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m out \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mdot(inputs, w, precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_bias:\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/base.py:688\u001b[0m, in \u001b[0;36mget_parameter\u001b[0;34m(name, shape, dtype, init)\u001b[0m\n\u001b[1;32m    685\u001b[0m param \u001b[38;5;241m=\u001b[39m check_not_none(param, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters cannot be `None`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(shape):\n\u001b[0;32m--> 688\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    689\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfq_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m with retrieved shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m does not match \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m param\n",
      "\u001b[0;31mValueError\u001b[0m: 'grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0/w' with retrieved shape (186, 512) does not match shape=[474, 512] dtype=dtype(bfloat16)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs, targets and forcings also go here. \n",
    "\n",
    "with open(\n",
    "    f'model/params/{graphcast_small}', 'rb'\n",
    ") as model:\n",
    "    ckpt = checkpoint.load(model, graphcast.CheckPoint)\n",
    "    params = ckpt.params\n",
    "    state = {}\n",
    "    model_config = ckpt.model_config\n",
    "    task_config = ckpt.task_config\n",
    "\n",
    "\n",
    "with open(r'model/stats/diffs_stddev_by_level.nc', 'rb') as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "with open(r'model/stats/mean_by_level.nc', 'rb') as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "with open(r'model/stats/stddev_by_level.nc', 'rb') as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config:graphcast.ModelConfig,\n",
    "    task_config:graphcast.TaskConfig\n",
    "):\n",
    "    predictor = graphcast.GraphCast(model_config, task_config)\n",
    "    \n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "    \n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "        predictor, diffs_stddev_by_level = \n",
    "        diffs_stddev_by_level, mean_by_level = \n",
    "        mean_by_level, stddev_by_level = stddev_by_level\n",
    "    )\n",
    "    \n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing = True)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    \n",
    "    return predictor(inputs, targets_template = targets_template, forcings = forcings)\n",
    "\n",
    "\n",
    "def with_configs(fn):\n",
    "    return functools.partial(\n",
    "                             fn, \n",
    "                             model_config = model_config,\n",
    "                             task_config = task_config\n",
    "                         )\n",
    "\n",
    "\n",
    "def with_params(fn):\n",
    "    return functools.partial(\n",
    "                             fn, \n",
    "                             params = params, \n",
    "                             state = state\n",
    "                         )\n",
    "\n",
    "\n",
    "def drop_state(fn):\n",
    "    return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    @classmethod\n",
    "    def predict(\n",
    "        cls,\n",
    "        inputs,\n",
    "        targets,\n",
    "        forcings\n",
    "    ) -> xarray.Dataset:\n",
    "        predictions = rollout.chunked_prediction(\n",
    "                                                 run_forward_jitted,\n",
    "                                                 rng = jax.random.PRNGKey(0),\n",
    "                                                 inputs = inputs,\n",
    "                                                 targets_template = targets,\n",
    "                                                 forcings = forcings\n",
    "                                             )\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # The code for creating inputs, targets, forcings & processing will be here. \n",
    "#     predictions = Predictor.predict(\n",
    "#                                     values['inputs'],\n",
    "#                                     values['targets'],\n",
    "#                                     values['forcings']\n",
    "#                                 )\n",
    "#     predictions.to_dataframe().to_csv('predictions.csv', sep = ',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The code for creating inputs, targets, forcings & processing will be here. \n",
    "predictions = Predictor.predict(\n",
    "                                values['inputs'],\n",
    "                                values['targets'],\n",
    "                                values['forcings']\n",
    "                            )\n",
    "# predictions.to_dataframe().to_csv('predictions.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ecf15-8b23-4c7f-8e51-e1390f3816b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1149d-8be5-4375-9bea-04a7d55a04c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c404f9-9428-42a4-b4b2-ef3a3fb8e89f",
   "metadata": {},
   "source": [
    "#### with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f729df-0af0-4877-8490-a6893863d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/20250113/GenCast/real_time/graphcast/rollout.py:295: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_target_steps = targets_template.dims[\"time\"]\n",
      "/home/jovyan/20250113/GenCast/real_time/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  scan_length = targets_template.dims['time']\n",
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n",
      "/home/jovyan/20250113/GenCast/real_time/graphcast/autoregressive.py:115: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_inputs = inputs.dims['time']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scan body function carry input and carry output must have the same pytree structure, but they differ:\n\nThe input carry component inputs[0] is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]'), 'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,))}) but the corresponding component of the carry output is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,)), 'level': <xarray.IndexVariable 'level' (level: 13)> Size: 104B\narray([  50.,  100.,  150.,  200.,  250.,  300.,  400.,  500.,  600.,  700.,\n        850.,  925., 1000.]), 'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]')}), so the pytree node metadata does not match.\n\nRevise the function so that the carry output has the same pytree structure as the carry input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:143\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:119\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(cls, inputs, targets, forcings)\u001b[0m\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/rollout.py:233\u001b[0m, in \u001b[0;36mchunked_prediction\u001b[0;34m(predictor_fn, rng, inputs, targets_template, forcings, num_steps_per_chunk, verbose)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Outputs a long trajectory by iteratively concatenating chunked predictions.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m chunks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction_chunk \u001b[38;5;129;01min\u001b[39;00m chunked_prediction_generator(\n\u001b[1;32m    234\u001b[0m     predictor_fn\u001b[38;5;241m=\u001b[39mpredictor_fn,\n\u001b[1;32m    235\u001b[0m     rng\u001b[38;5;241m=\u001b[39mrng,\n\u001b[1;32m    236\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    237\u001b[0m     targets_template\u001b[38;5;241m=\u001b[39mtargets_template,\n\u001b[1;32m    238\u001b[0m     forcings\u001b[38;5;241m=\u001b[39mforcings,\n\u001b[1;32m    239\u001b[0m     num_steps_per_chunk\u001b[38;5;241m=\u001b[39mnum_steps_per_chunk,\n\u001b[1;32m    240\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose):\n\u001b[1;32m    241\u001b[0m   chunks_list\u001b[38;5;241m.\u001b[39mappend(jax\u001b[38;5;241m.\u001b[39mdevice_get(prediction_chunk))\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xarray\u001b[38;5;241m.\u001b[39mconcat(chunks_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/rollout.py:345\u001b[0m, in \u001b[0;36mchunked_prediction_generator\u001b[0;34m(predictor_fn, rng, inputs, targets_template, forcings, num_steps_per_chunk, verbose, pmap_devices)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Make predictions for the chunk.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m rng, this_rng \u001b[38;5;241m=\u001b[39m split_rng_fn(rng)\n\u001b[0;32m--> 345\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_targets_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforcings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_forcings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# In the pmapped case, profiling reveals that the predictions, forcings and\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# inputs are all copied onto a single TPU, causing OOM. To avoid this\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# we pull all of the input/output data off the devices. This will have\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# some performance impact, but maximise the memory efficiency.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# TODO(aelkadi): Pmap `_get_next_inputs` when running under pmap, and\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# remove the device_get.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pmap_devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<timed exec>:105\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(**kw)\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/transform.py:456\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(params\u001b[38;5;241m=\u001b[39mparams, state\u001b[38;5;241m=\u001b[39mstate, rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m<timed exec>:85\u001b[0m, in \u001b[0;36mrun_forward\u001b[0;34m(model_config, task_config, inputs, targets_template, forcings)\u001b[0m\n",
      "File \u001b[0;32m~/20250113/GenCast/real_time/graphcast/autoregressive.py:212\u001b[0m, in \u001b[0;36mPredictor.__call__\u001b[0;34m(self, inputs, targets_template, forcings, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     one_step_prediction \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mremat(one_step_prediction)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Loop (without unroll) with hk states in cell (jax.lax.scan won't do).\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m _, flat_preds \u001b[38;5;241m=\u001b[39m \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_step_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# The result of scan will have an extra leading axis on all arrays,\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# corresponding to the target times in this case. We need to be prepared for\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# it when unflattening the arrays back into a Dataset:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m scan_result_template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    218\u001b[0m     target_template\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;241m.\u001b[39mexpand_dims(time\u001b[38;5;241m=\u001b[39mtargets_template\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/haiku/_src/stateful.py:643\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length, reverse, unroll)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# We know that we don't need to thread params in and out, since for init we\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# have already created them (given that above we unroll one step of the scan)\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# and for apply we know they are immutable. As such we only need to thread the\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# state and rng in and out.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m init \u001b[38;5;241m=\u001b[39m (init, internal_state(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 643\u001b[0m (carry, state), ys \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstateful_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m update_internal_state(state)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_init_fn:\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.mlspace/envs/gencast_env/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:400\u001b[0m, in \u001b[0;36m_check_carry_type\u001b[0;34m(name, body_fun, in_carry, out_carry_tree, out_avals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m       differences \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m diffs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    399\u001b[0m                      \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiffs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 400\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m function carry input and carry output must have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytree structure, but they differ:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdifferences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevise the function so that the carry output has the same pytree \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructure as the carry input.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_map(core\u001b[38;5;241m.\u001b[39mtypematch, in_avals, out_avals)):\n\u001b[1;32m    407\u001b[0m   diffs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_aval\u001b[38;5;241m.\u001b[39mstr_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but the corresponding output carry component has type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    409\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_aval\u001b[38;5;241m.\u001b[39mstr_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_aval_mismatch_extra(in_aval,\u001b[38;5;250m \u001b[39mout_aval)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    410\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m path, in_aval, out_aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(paths, in_avals, out_avals)\n\u001b[1;32m    411\u001b[0m            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mtypematch(in_aval, out_aval)]\n",
      "\u001b[0;31mTypeError\u001b[0m: scan body function carry input and carry output must have the same pytree structure, but they differ:\n\nThe input carry component inputs[0] is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]'), 'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,))}) but the corresponding component of the carry output is a <class 'xarray.core.dataset.Dataset'> with pytree metadata _HashableCoords({'lat': <xarray.IndexVariable 'lat' (lat: 181)> Size: 1kB\narray([-90., -89., -88., -87., -86., -85., -84., -83., -82., -81., -80., -79.,\n       -78., -77., -76., -75., -74., -73., -72., -71., -70., -69., -68., -67.,\n       -66., -65., -64., -63., -62., -61., -60., -59., -58., -57., -56., -55.,\n       -54., -53., -52., -51., -50., -49., -48., -47., -46., -45., -44., -43.,\n       -42., -41., -40., -39., -38., -37., -36., -35., -34., -33., -32., -31.,\n       -30., -29., -28., -27., -26., -25., -24., -23., -22., -21., -20., -19.,\n       -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,\n        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,\n         6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,\n        30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,\n        42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,\n        54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,\n        90.]), 'lon': <xarray.IndexVariable 'lon' (lon: 360)> Size: 3kB\narray([  0.,   1.,   2., ..., 357., 358., 359.], shape=(360,)), 'level': <xarray.IndexVariable 'level' (level: 13)> Size: 104B\narray([  50.,  100.,  150.,  200.,  250.,  300.,  400.,  500.,  600.,  700.,\n        850.,  925., 1000.]), 'time': <xarray.IndexVariable 'time' (time: 2)> Size: 16B\narray(['2024-01-01T06:00:00.000000000', '2024-01-01T12:00:00.000000000'],\n      dtype='datetime64[ns]')}), so the pytree node metadata does not match.\n\nRevise the function so that the carry output has the same pytree structure as the carry input."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Includes the packages imported and constants assigned. \n",
    "# The functions created for the inputs, targets and forcings also go here. \n",
    "\n",
    "with open(\n",
    "    f'model/params/{graphcast_small}', 'rb'\n",
    ") as model:\n",
    "    ckpt = checkpoint.load(model, graphcast.CheckPoint)\n",
    "    params = ckpt.params\n",
    "    state = {}\n",
    "    model_config = ckpt.model_config\n",
    "    task_config = ckpt.task_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#!!!!!!!!PADDING!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "existing_array = params['grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0']['w']\n",
    "\n",
    "zeros_array = np.zeros((288, 512), dtype=np.float32)  \n",
    "\n",
    "new_array = np.vstack((existing_array, zeros_array))\n",
    "\n",
    "params['grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0']['w'] = new_array\n",
    "\n",
    "del existing_array, zeros_array, new_array\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "existing_array = params['grid2mesh_gnn/~_networks_builder/encoder_nodes_mesh_nodes_mlp/~/linear_0']['w']\n",
    "\n",
    "zeros_array = np.zeros((288, 512), dtype=np.float32)  \n",
    "\n",
    "new_array = np.vstack((existing_array, zeros_array))\n",
    "\n",
    "params['grid2mesh_gnn/~_networks_builder/encoder_nodes_mesh_nodes_mlp/~/linear_0']['w'] = new_array\n",
    "\n",
    "del existing_array, zeros_array, new_array\n",
    "gc.collect()\n",
    "\n",
    "#!!!!!!!!PADDING!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(r'model/stats/diffs_stddev_by_level.nc', 'rb') as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "with open(r'model/stats/mean_by_level.nc', 'rb') as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "with open(r'model/stats/stddev_by_level.nc', 'rb') as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config:graphcast.ModelConfig,\n",
    "    task_config:graphcast.TaskConfig\n",
    "):\n",
    "    predictor = graphcast.GraphCast(model_config, task_config)\n",
    "    \n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "    \n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "        predictor, diffs_stddev_by_level = \n",
    "        diffs_stddev_by_level, mean_by_level = \n",
    "        mean_by_level, stddev_by_level = stddev_by_level\n",
    "    )\n",
    "    \n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing = True)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    \n",
    "    return predictor(inputs, targets_template = targets_template, forcings = forcings)\n",
    "\n",
    "\n",
    "def with_configs(fn):\n",
    "    return functools.partial(\n",
    "                             fn, \n",
    "                             model_config = model_config,\n",
    "                             task_config = task_config\n",
    "                         )\n",
    "\n",
    "\n",
    "def with_params(fn):\n",
    "    return functools.partial(\n",
    "                             fn, \n",
    "                             params = params, \n",
    "                             state = state\n",
    "                         )\n",
    "\n",
    "\n",
    "def drop_state(fn):\n",
    "    return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    @classmethod\n",
    "    def predict(\n",
    "        cls,\n",
    "        inputs,\n",
    "        targets,\n",
    "        forcings\n",
    "    ) -> xarray.Dataset:\n",
    "        predictions = rollout.chunked_prediction(\n",
    "                                                 run_forward_jitted,\n",
    "                                                 rng = jax.random.PRNGKey(0),\n",
    "                                                 inputs = inputs,\n",
    "                                                 targets_template = targets,\n",
    "                                                 forcings = forcings\n",
    "                                             )\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # The code for creating inputs, targets, forcings & processing will be here. \n",
    "#     predictions = Predictor.predict(\n",
    "#                                     values['inputs'],\n",
    "#                                     values['targets'],\n",
    "#                                     values['forcings']\n",
    "#                                 )\n",
    "#     predictions.to_dataframe().to_csv('predictions.csv', sep = ',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The code for creating inputs, targets, forcings & processing will be here. \n",
    "predictions = Predictor.predict(\n",
    "                                values['inputs'],\n",
    "                                values['targets'],\n",
    "                                values['forcings']\n",
    "                            )\n",
    "# predictions.to_dataframe().to_csv('predictions.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03fa6ee2-9a88-49c2-809c-c29a56733b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2516d4e-81be-4b50-9734-522639e3e160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ffd45-668f-45c4-8b76-053292b18a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc585855-efff-493d-917e-5d72a69ff706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663ecff-2863-4911-a70c-b7097954a9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0904095-364e-43d7-9746-d50807d601d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbcf42-13ae-4414-8e52-d50cfbd72e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d0fa6-09ba-4037-9f16-dcae94aeda18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ebebea-b21d-4226-962e-4c511c20f8df",
   "metadata": {},
   "source": [
    "ValueError: 'grid2mesh_gnn/~_networks_builder/encoder_nodes_grid_nodes_mlp/~/linear_0/w' with retrieved shape (186, 512) does not match shape=[474, 512] dtype=dtype(bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a30967-07ac-4ddc-ad0a-222dc49e96f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39ae85-ad21-4a0a-94a3-ffa0816373b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24700f-37ae-4a33-ba2a-85b61ee18283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a4ec8-87f8-4b31-b628-8440a890127a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146e536-031a-43f7-b857-85c9362c609b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb161d0-643a-44ce-9673-5c386ab3b914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d222fd-4e3f-4301-aaa4-e654793c235a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799f812-6dab-43eb-bf2f-10caf6109cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf3fdb-c375-4b0a-935c-a148ff2c8d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-gencast_env]",
   "language": "python",
   "name": "conda-env-.mlspace-gencast_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
